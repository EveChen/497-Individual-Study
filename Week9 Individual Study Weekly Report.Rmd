---
title: "Machine Learning Week 9 Report"
author: "Yuan Yi Chen (Eve)"
date: "2017/6/2"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r, include = F}
setwd("C:/Users/Eve/Dropbox/UCLA Files/Courses/497 Individual Study")
```
#**Agenda**

###**A. Brief introduction of What I learn this week**
  * ####Before this Week: 
    + Know how to use **glm**, **glmnet** and **h20** functions to run logistic regression
    + Know in what circumstance should I apply logistic regression model
  * ####After this Week:
    + Know how to use **randomForest package**, **h20 package** and **XGBoost package** functions to run logistic regression
    + Know in what circumstance should I apply random forest model
  * ####Plan for Next Week:
    + Study Basic Neural Network
    + Apply Neural Network to MNist data set with **h2o package** and **Tensorflow**
    
###**B. Random Forest Codes: Using R, H2O and XGBoost packages**
  * ####R - h2o package
  * ####R - XGBoost package
  * ####R - randomForest package
  

***

###A. Brief introduction of What I learn this week

####Random forest is a popular method to solve different kinds of machine learning tasks. Due to it's high accuracy, it seems to me that it's more prone to a predictive model instead of a description model. Basically, random forest does two things:
  * Randomly choose different features to build different trees
  * Calculate the average value of all the trees it created previously

####After we apply random forest model to the test data set, We usually can build up confusion matrix, AUC, ROC curve and variable importance to see how does our model perform. Also, there is no limitation about how many trees that we can build. The only concern is the computability of our desktop. That is, random forest seldom overfits in our training set.(Usually, an overfitting will happen if we have only few features to train.) 

####The general steps are
  * Manipulate our data set (attention to features which are factors)
  * Train the random forest model
  * Predict a test data set
  * View the accuracy, confusion matrix, variable importance
  * Rebuild the random forest model based on the fourth step
  * Predict our test data set again
  * Check the accuracy, confusion matrix, partial dependence
  * Apply other methods to double check if they all come to the same conclusion
  

####Here are some pros and cons that I saw from the textbook and Quora
Random Forest Model           
----|-----------------------------------------------------------
Pros| 
    | 1. Fast to train our training data
    | 2. High accuracy
    | 3. Automatically Generalized our data within the process
Cons| 
    | 1. Inconsistent - because it's classification method
    | 2. Hard to interpret the results
    | 3. Slow to predict our testing data
    | 4. Need high computability, which may be expensive
    

***


####Same as the logistic regression I did last week, I would like to take a look about our data set before I use **R**, **H2O** and **XGBoost** to run random forest models.

```{r, include = F}
###1. Setup
#Remove Objects
rm(list=ls())

#Clear Memory
gc(reset=TRUE)

#Set Working Directory
setwd("C:/Users/Eve/Dropbox/UCLA Files/Courses/497 Individual Study")
```

###1. Load Packages
```{r, message = F}
library(readr)   #Use to read data
library(glmnet)  #Use to apply logistic regression
library(ROCR)    #Use to calcuate AUC
library(h2o)     #Use to run logistic regression and random forest 
library(xgboost) #Use to run random forest model
library(randomForest) #Use to run random forest model
```

###2. Load data set
####This **bank marketing** data set is provided by a Portuguese banking institution. It takes records on every phone calls they make to promote their product - term deposit. Our goal is to predict if a customer will subsribe a product after they make phone calls.

```{r}
dat <- read.csv("bank.csv")
str(dat)
```

####We totally have 45211 observations, 15 input variables and 1 output variables (y).

```{r}
head(dat, 3)
```



***

###B. Random Forest Model
  * Using h2o package
  * Using XGBoost package
  * Using randomForest package

###1. Using h2o package
####Step1: Initiate h2o package
```{r}
h2o.init(nthreads=-1)
```

####Step2: Load data set
```{r}
dx <- h2o.importFile("bank.csv")
```

####Step3: Split into training and test data sets (*h2o will automatically shuffle and generalize our data*)
```{r}
dx_split <- h2o.splitFrame(dx, ratios = 0.6, seed = 123)
dx_train <- dx_split[[1]]
dx_test <- dx_split[[2]]

Xnames <- names(dx_train)[which(names(dx_train)!="y")]
```


####Step4: Apply a random forest model 
```{r}
system.time({
  md_h2o <- h2o.randomForest(x = Xnames, y = "y", training_frame = dx_train, ntrees = 200)
})
```

####Step5: Prediction & AUC
```{r}
auc_h2o <- h2o.auc(h2o.performance(md_h2o, dx_test))
auc_h2o
```


***

###Brief conclusion for h2o package
####


***

###2. Using XGBoost package

####Step1: Load our data set 
```{r, message = F}
d <- read_csv("bank.csv")
```

####Step2: Shuffle our data and split into training and test data set

```{r}
set.seed(123)
N <- nrow(d)
idx <- sample(1:N, 0.6*N)
d_train <- d[idx,]
d_test <- d[-idx,]
```

####Step3: Sparse our original data to make categorical features become numeric features
```{r}
X <- Matrix::sparse.model.matrix(y ~ . - 1, data = d)
X_train <- X[idx,]
X_test <- X[-idx,]
```

####Step4: Apply random forest model 
```{r}
system.time({
  n_proc <- parallel::detectCores()
  md_xgboost <- xgboost(data = X_train, label = ifelse(d_train$y=='Y',1,0),
                nthread = n_proc, nround = 1, max_depth = 20,
                num_parallel_tree = 200, subsample = 0.632,
                colsample_bytree = 1/sqrt(length(X_train@x)/nrow(X_train)),
                save_period = NULL)
})
```
####Step5: Make prediction and calculate AUC
```{r}
phat_xgboost <- predict(md_xgboost, newdata = X_test)
rocr_pred_xgboost <- prediction(phat_xgboost, d_test$y)
AUC_xgboost <- performance(rocr_pred_xgboost, "auc")@y.values[[1]]
AUC_xgboost
```


***


